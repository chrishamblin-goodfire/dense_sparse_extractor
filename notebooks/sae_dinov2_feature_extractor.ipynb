{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SAE feature extractor on DINOv2 patch activations\n",
        "\n",
        "This notebook demonstrates a simple end-to-end pipeline:\n",
        "\n",
        "- Load a DINOv2 ViT backbone **from a local `dinov2/` checkout** via `torch.hub.load(..., source=\\\"local\\\")`\n",
        "- Run a small batch of images through the model and extract **patch-token activations**\n",
        "- Train a small **Top-k Sparse Autoencoder (SAE)** using `overcomplete`\n",
        "- Use the SAE codes as a sparse feature representation\n",
        "\n",
        "### Assumptions\n",
        "\n",
        "- You have a sibling checkout at `../dinov2` (relative to this repo root).\n",
        "- You installed this repo requirements (see `requirements.txt`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import FakeData\n",
        "from torchvision import transforms\n",
        "\n",
        "from overcomplete.sae import TopKSAE, train_sae\n",
        "\n",
        "\n",
        "def get_projects_dir() -> Path:\n",
        "    # If you run this notebook from the repo root, this resolves to ../\n",
        "    cwd = Path.cwd().resolve()\n",
        "    if cwd.name == \"dense_sparse_extractor\":\n",
        "        return cwd.parent\n",
        "    # Fallback: walk up until we find this repo root, then take its parent.\n",
        "    for p in [cwd, *cwd.parents]:\n",
        "        if (p / \"dense_sparse_extractor\").is_dir() and (p / \"dinov2\").is_dir():\n",
        "            return p\n",
        "        if (p / \"pyproject.toml\").is_file() and (p / \"dense_sparse_extractor\").is_dir():\n",
        "            return p.parent\n",
        "    raise RuntimeError(\"Could not infer projects dir. Run from the repo root.\")\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load DINOv2 from a local checkout via torch.hubprojects_dir = get_projects_dir()dinov2_dir = projects_dir / \"dinov2\"if not dinov2_dir.is_dir():    raise FileNotFoundError(f\"Expected dinov2 checkout at: {dinov2_dir}\")# Use a small backbone for the demo.# If you don't have network access (or weights aren't cached), this will fall back to pretrained=False.try:    model = torch.hub.load(str(dinov2_dir), \"dinov2_vits14\", source=\"local\", pretrained=True)except Exception as e:    print(\"[warn] Failed to load pretrained weights; falling back to pretrained=False.\")    print(\"       Error:\", repr(e))    model = torch.hub.load(str(dinov2_dir), \"dinov2_vits14\", source=\"local\", pretrained=False)model.eval()model.to(device)# Fake images (no downloads). Normalize like ImageNet.transform = transforms.Compose(    [        transforms.Resize((224, 224)),        transforms.ToTensor(),        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),    ])dataset = FakeData(size=32, image_size=(3, 224, 224), num_classes=10, transform=transform)loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=0)batch = next(iter(loader))images, labels = batchimages = images.to(device)images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract patch-token activations from DINOv2\n",
        "# forward_features returns a dict with (B, N, D) patch tokens under x_norm_patchtokens.\n",
        "with torch.no_grad():\n",
        "    feats = model.forward_features(images)\n",
        "    patch_tokens = feats[\"x_norm_patchtokens\"]  # (B, N, D)\n",
        "\n",
        "B, N, D = patch_tokens.shape\n",
        "activations = patch_tokens.reshape(B * N, D).contiguous()\n",
        "patch_tokens.shape, activations.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a small TopKSAE on these activations (toy demo settings)\n",
        "sae = TopKSAE(input_shape=D, nb_concepts=512, top_k=16, device=device)\n",
        "\n",
        "act_loader = DataLoader(activations, batch_size=1024, shuffle=True)\n",
        "opt = torch.optim.Adam(sae.parameters(), lr=5e-4)\n",
        "\n",
        "def mse_criterion(x, x_hat, pre_codes, codes, dictionary):\n",
        "    return (x - x_hat).pow(2).mean()\n",
        "\n",
        "logs = train_sae(sae, act_loader, mse_criterion, opt, nb_epochs=2, device=device)\n",
        "\n",
        "# Use SAE as a sparse feature extractor\n",
        "sae.eval()\n",
        "with torch.no_grad():\n",
        "    _, codes = sae.encode(activations)  # (B*N, nb_concepts) with top_k non-zeros\n",
        "\n",
        "codes.shape, (codes != 0).float().sum(dim=-1).mean().item()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Dense Sparse Extractor",
      "language": "python",
      "name": "dense_sparse_extractor"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
